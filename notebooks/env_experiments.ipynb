{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#addition module dir to path\n",
    "module_dir = os.path.split( os.getcwd() )[:-1][0]\n",
    "sys.path.append(module_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = os.getcwd()\n",
    "data_dir = '/Users/albelyakov/Data/rl_warehouse'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.envs import wh_map as wm\n",
    "from src.envs import wh_objects as wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import readline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_map(map_obj, agent_obj):\n",
    "#     os.system('clear')\n",
    "#     subprocess.call('reset')\n",
    "    readline.clear_history()\n",
    "    for i, row in enumerate(map_obj):\n",
    "        to_print = list()\n",
    "        for j, obj in enumerate(row):\n",
    "            if (i, j) == agent_obj.coordinates:\n",
    "                to_print.append(agent_obj.sprite)\n",
    "            else:\n",
    "                to_print.append(obj.sprite)\n",
    "        print(''.join(to_print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_obj = wm.init_wh_map(wm.wh_vis_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++\n",
      "+.#..#..#..#..#..#.+\n",
      "+.#..#..#..#..#..#.+\n",
      "+.#..#..#..#..#..#.+\n",
      "+.#..#..#..#..#..#.+\n",
      "+..................+\n",
      "+..................+\n",
      "+.#######..#######.+\n",
      "+..................+\n",
      "+..................+\n",
      "+.#######..#######.+\n",
      "+..................+\n",
      "+..................+\n",
      "+.##..##..##..##..#+\n",
      "+.##..##..##..##..#+\n",
      "+.##..##..##..##..#+\n",
      "+.##..##..##..##..#+\n",
      "+..................+\n",
      "+..................+\n",
      "+..................+\n",
      "+$$$$$$$$$$$$$$$$$$+\n"
     ]
    }
   ],
   "source": [
    "render_map(map_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = wo.Agent(\n",
    "    coordinates=(18,9)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++\n",
      "+.#..#..#..#..#..#.+\n",
      "+.#..#..#..#..#..#.+\n",
      "+.#..#..#..#..#..#.+\n",
      "+.#..#..#..#..#..#.+\n",
      "+..................+\n",
      "+..................+\n",
      "+.#######..#######.+\n",
      "+..................+\n",
      "+..................+\n",
      "+.#######..#######.+\n",
      "+..................+\n",
      "+..................+\n",
      "+.##..##..##..##..#+\n",
      "+.##..##..##..##..#+\n",
      "+.##..##..##..##..#+\n",
      "+.##..##..##..##..#+\n",
      "+..................+\n",
      "+........X.........+\n",
      "+..................+\n",
      "+$$$$$$$$$$$$$$$$$$+\n"
     ]
    }
   ],
   "source": [
    "render_map(map_obj, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_loop():\n",
    "    map_obj = wm.init_wh_map(wm.wh_vis_map)\n",
    "    agent_obj = wo.Agent(\n",
    "        coordinates=(18,9)\n",
    "    )\n",
    "    availible_actions = set(['w', 'a', 's', 'd', 'q', 't', 'g', 'i', 'r'])\n",
    "    score = 0\n",
    "    render_map(map_obj, agent_obj)\n",
    "    while True:\n",
    "        while True:\n",
    "            action = input()\n",
    "            if action in availible_actions:\n",
    "                break\n",
    "        if action == 'w':\n",
    "            r = agent_obj.move(to='u',map_obj=map_obj)\n",
    "            if r == 0:\n",
    "                score -= 10\n",
    "        elif action == 'a':\n",
    "            r = agent_obj.move(to='l',map_obj=map_obj)\n",
    "            if r == 0:\n",
    "                score -= 10\n",
    "        elif action == 's':\n",
    "            r = agent_obj.move(to='d',map_obj=map_obj)\n",
    "            if r == 0:\n",
    "                score -= 10\n",
    "        elif action == 'd':\n",
    "            r = agent_obj.move(to='r',map_obj=map_obj)\n",
    "            if r == 0:\n",
    "                score -= 10\n",
    "        elif action == 'q':\n",
    "            print('Breaking simulation.')\n",
    "            break\n",
    "        elif action == 't':\n",
    "            r = agent_obj.take_product(product_name='MacBookPro', map_obj=map_obj)\n",
    "            if r == 0:\n",
    "                score -= 10\n",
    "        elif action == 'g':\n",
    "            r = agent_obj.put_product(product_name='MacBookPro', map_obj=map_obj)\n",
    "            if r == 0:\n",
    "                score -= 10\n",
    "            elif r == -1:\n",
    "                score -= 1000\n",
    "            elif r == 10:\n",
    "                score += 500\n",
    "        elif action == 'i':\n",
    "            r = agent_obj.inspect_shelf(map_obj=map_obj)\n",
    "            if r == 0:\n",
    "                score -= 10\n",
    "            else:\n",
    "                print(r)\n",
    "        elif action == 'w':\n",
    "            print('Waiting...')\n",
    "        score -= 10\n",
    "        render_map(map_obj, agent_obj)\n",
    "        print(f'Score: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.envs.wh_env import WarehouseEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WarehouseEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 1000\n",
      "Penalties incurred: 391\n"
     ]
    }
   ],
   "source": [
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward < -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.05)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training q-learning agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_state(state, encoder):\n",
    "    state = tuple(state)\n",
    "    if state not in encoder:\n",
    "        encoder[state] = len(encoder)\n",
    "    return encoder[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 26min 19s, sys: 1min 34s, total: 27min 54s\n",
      "Wall time: 26min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "q_table = np.zeros([18 * 18 * 4, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.2\n",
    "n_epoch = 100000\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "env = WarehouseEnv()\n",
    "encoder = dict()\n",
    "for i in range(1, n_epoch + 1):\n",
    "    state = env.reset()\n",
    "    state = encode_state(state, encoder)\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_state = encode_state(next_state, encoder)\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        \n",
    "        if reward < -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296, 8)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 1000.0\n",
      "Average penalties per episode: 999.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = encode_state(state, encoder)\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "#         print(state, action)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        state = encode_state(state, encoder)\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 1000\n",
      "Penalties incurred: 0\n"
     ]
    }
   ],
   "source": [
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "state = env.reset()\n",
    "state = encode_state(state, encoder)\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    state = encode_state(state, encoder)\n",
    "    if reward < -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++\n",
      "+.#..#..#..#..#..#.+\n",
      "+.#..#..#X.#..#..#.+\n",
      "+.#..#..#..#..#..#.+\n",
      "+.#..#..#..#..#..#.+\n",
      "+..................+\n",
      "+..................+\n",
      "+.#######..#######.+\n",
      "+..................+\n",
      "+..................+\n",
      "+.#######..#######.+\n",
      "+..................+\n",
      "+..................+\n",
      "+.##..##..##..##..#+\n",
      "+.##..##..##..##..#+\n",
      "+.##..##..##..##..#+\n",
      "+.##..##..##..##..#+\n",
      "+..................+\n",
      "+..................+\n",
      "+..................+\n",
      "+$$$$$$$$$$$$$$$$$$+\n",
      "Timestep: 1000\n",
      "State: 285\n",
      "Action: 2\n",
      "Reward: -10\n"
     ]
    }
   ],
   "source": [
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.05)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.q_table import QTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WarehouseEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QTable(\n",
    "    environment=env, \n",
    "    verbose=True,\n",
    "    alpha=0.25,\n",
    "    gamma=0.9,\n",
    "    epsilon=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [18:31<00:00, 36.00it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(n_epoch=40000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 1000.0\n",
      "Average penalties per episode: 999.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_timesteps': 1000.0, 'avg_penalties': 999.0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.verbose=True\n",
    "model.evaluate_performance(episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 1000\n",
      "Penalties incurred: 0\n"
     ]
    }
   ],
   "source": [
    "model.operate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++\n",
      "+.#..#..#..#..#..#.+\n",
      "+.#..#..#..#..#..#.+\n",
      "+.#..#..#..#..#..#.+\n",
      "+.#..#..#..#..#..#.+\n",
      "+..................+\n",
      "+..................+\n",
      "+.#######..#######.+\n",
      "+..................+\n",
      "+..................+\n",
      "+.#######..#######.+\n",
      "+..................+\n",
      "+..................+\n",
      "+.##..##..##..##..#+\n",
      "+.##..##..##..##..#+\n",
      "+.##..##..##..##..#+\n",
      "+.##..##..##..##..#+\n",
      "+..................+\n",
      "+..................+\n",
      "+.........X........+\n",
      "+$$$$$$$$$$$$$$$$$$+\n",
      "Timestep: 401\n",
      "State: 666\n",
      "Action: 7\n",
      "Reward: -10\n"
     ]
    }
   ],
   "source": [
    "model.show_operation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
